{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANP_MONTHLY.csv: 5642 entries\n",
      "AUSTRALIAN_ELECTRICITY_DEMAND_DATASET.csv: 20 entries\n",
      "M4_HOURLY_DATASET.csv: 1652 entries\n",
      "M4_WEEKLY_DATASET.csv: 1436 entries\n",
      "NN5_DAILY_DATASET_WITHOUT_MISSING_VALUES.csv: 444 entries\n",
      "NN5_WEEKLY_DATASET.csv: 444 entries\n",
      "Erro ao ler ./timeseries/mestrado/resultados/DWT_catboost/normal/ETTh1.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/DWT_catboost/normal/ETTh1.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/DWT_catboost/normal/ETTh2.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/DWT_catboost/normal/ETTh2.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/DWT_catboost/normal/ETTm1.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/DWT_catboost/normal/ETTm1.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/DWT_catboost/normal/ETTm2.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/DWT_catboost/normal/ETTm2.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "\n",
    "model = \"DWT_catboost\"\n",
    "part = '/normal'\n",
    "csv_files = [\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ANP_MONTHLY.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/AUSTRALIAN_ELECTRICITY_DEMAND_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/M4_HOURLY_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/M4_WEEKLY_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/NN5_DAILY_DATASET_WITHOUT_MISSING_VALUES.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/NN5_WEEKLY_DATASET.csv\",\n",
    "    # f\"./timeseries/mestrado/resultados/{model}{part}/PEDESTRIAN_COUNTS_DATASET.csv\",\n",
    "    # f\"./timeseries/mestrado/resultados/{model}{part}/US_BIRTHS_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTh1.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTh2.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTm1.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTm2.csv\",\n",
    "]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, sep=\";\")\n",
    "        print(f\"{csv_file.split('/')[-1]}: {len(df)} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler {csv_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_index</th>\n",
       "      <th>horizon</th>\n",
       "      <th>regressor</th>\n",
       "      <th>mape</th>\n",
       "      <th>pocid</th>\n",
       "      <th>smape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>msmape</th>\n",
       "      <th>mae</th>\n",
       "      <th>test</th>\n",
       "      <th>predictions</th>\n",
       "      <th>start_test</th>\n",
       "      <th>final_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>48.9362</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>1331.0723</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>1057.9560</td>\n",
       "      <td>[5423.510184, 5145.417668, 4831.310656, 4601.1...</td>\n",
       "      <td>[7495.5086221054435, 7481.106812395642, 7524.6...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2015-02-28 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>48.9362</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1375.5340</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1065.5747</td>\n",
       "      <td>[5356.569396, 5119.204704, 4792.671786, 4548.7...</td>\n",
       "      <td>[7865.908512930311, 7793.200758871765, 7715.92...</td>\n",
       "      <td>2015-02-27</td>\n",
       "      <td>2015-02-27 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>48.9362</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>1272.6834</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>953.4950</td>\n",
       "      <td>[5293.779442, 5051.24703, 4744.092516, 4522.20...</td>\n",
       "      <td>[7501.148012649284, 7420.0160014048115, 7364.0...</td>\n",
       "      <td>2015-02-26</td>\n",
       "      <td>2015-02-26 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>51.0638</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>1234.0361</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>960.7877</td>\n",
       "      <td>[5326.979198, 5117.6083, 4818.190136, 4599.993...</td>\n",
       "      <td>[7445.0415444196815, 7359.201416802917, 7311.9...</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-25 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1696</td>\n",
       "      <td>57.4468</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>1301.9550</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>1036.4776</td>\n",
       "      <td>[5507.45532, 5289.4003, 5026.337266, 4811.4675...</td>\n",
       "      <td>[7813.464354804712, 7678.828528317668, 7633.04...</td>\n",
       "      <td>2015-02-24</td>\n",
       "      <td>2015-02-24 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>46.8085</td>\n",
       "      <td>0.1964</td>\n",
       "      <td>1703.6190</td>\n",
       "      <td>0.1964</td>\n",
       "      <td>1322.2391</td>\n",
       "      <td>[5040.995406, 4892.438998, 4644.685104, 4478.5...</td>\n",
       "      <td>[8397.78872628142, 8315.456222450852, 8409.035...</td>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>2015-02-23 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>40.4255</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>1689.4424</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>1393.2459</td>\n",
       "      <td>[5061.18634, 4860.166678, 4592.798732, 4387.37...</td>\n",
       "      <td>[7869.347245088073, 7781.153254999102, 7768.32...</td>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>2015-02-22 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>NaiveSeasonal_normal</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>55.3191</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>1161.9640</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>930.6673</td>\n",
       "      <td>[5494.511182, 5229.571906, 4951.599026, 4683.6...</td>\n",
       "      <td>[7284.027574353003, 7190.7797158871745, 7245.1...</td>\n",
       "      <td>2015-02-21</td>\n",
       "      <td>2015-02-21 23:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_index  horizon             regressor   mape   pocid  smape  \\\n",
       "0               0       48  NaiveSeasonal_normal 0.1976 48.9362 0.1724   \n",
       "6               0       48  NaiveSeasonal_normal 0.1833 48.9362 0.1660   \n",
       "11              0       48  NaiveSeasonal_normal 0.1656 48.9362 0.1529   \n",
       "16              0       48  NaiveSeasonal_normal 0.1640 51.0638 0.1547   \n",
       "20              0       48  NaiveSeasonal_normal 0.1696 57.4468 0.1596   \n",
       "25              0       48  NaiveSeasonal_normal 0.2280 46.8085 0.1964   \n",
       "30              0       48  NaiveSeasonal_normal 0.2722 40.4255 0.2252   \n",
       "35              0       48  NaiveSeasonal_normal 0.1747 55.3191 0.1541   \n",
       "\n",
       "        rmse  msmape       mae  \\\n",
       "0  1331.0723  0.1724 1057.9560   \n",
       "6  1375.5340  0.1660 1065.5747   \n",
       "11 1272.6834  0.1529  953.4950   \n",
       "16 1234.0361  0.1547  960.7877   \n",
       "20 1301.9550  0.1596 1036.4776   \n",
       "25 1703.6190  0.1964 1322.2391   \n",
       "30 1689.4424  0.2252 1393.2459   \n",
       "35 1161.9640  0.1541  930.6673   \n",
       "\n",
       "                                                 test  \\\n",
       "0   [5423.510184, 5145.417668, 4831.310656, 4601.1...   \n",
       "6   [5356.569396, 5119.204704, 4792.671786, 4548.7...   \n",
       "11  [5293.779442, 5051.24703, 4744.092516, 4522.20...   \n",
       "16  [5326.979198, 5117.6083, 4818.190136, 4599.993...   \n",
       "20  [5507.45532, 5289.4003, 5026.337266, 4811.4675...   \n",
       "25  [5040.995406, 4892.438998, 4644.685104, 4478.5...   \n",
       "30  [5061.18634, 4860.166678, 4592.798732, 4387.37...   \n",
       "35  [5494.511182, 5229.571906, 4951.599026, 4683.6...   \n",
       "\n",
       "                                          predictions  start_test  \\\n",
       "0   [7495.5086221054435, 7481.106812395642, 7524.6...  2015-02-28   \n",
       "6   [7865.908512930311, 7793.200758871765, 7715.92...  2015-02-27   \n",
       "11  [7501.148012649284, 7420.0160014048115, 7364.0...  2015-02-26   \n",
       "16  [7445.0415444196815, 7359.201416802917, 7311.9...  2015-02-25   \n",
       "20  [7813.464354804712, 7678.828528317668, 7633.04...  2015-02-24   \n",
       "25  [8397.78872628142, 8315.456222450852, 8409.035...  2015-02-23   \n",
       "30  [7869.347245088073, 7781.153254999102, 7768.32...  2015-02-22   \n",
       "35  [7284.027574353003, 7190.7797158871745, 7245.1...  2015-02-21   \n",
       "\n",
       "             final_test  \n",
       "0   2015-02-28 23:30:00  \n",
       "6   2015-02-27 23:30:00  \n",
       "11  2015-02-26 23:30:00  \n",
       "16  2015-02-25 23:30:00  \n",
       "20  2015-02-24 23:30:00  \n",
       "25  2015-02-23 23:30:00  \n",
       "30  2015-02-22 23:30:00  \n",
       "35  2015-02-21 23:30:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "df_agent = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/catboost/normal/M4_HOURLY_DATASET.csv\",\n",
    "    sep=\";\",\n",
    "    )\n",
    "\n",
    "df_agent[df_agent['dataset_index'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Models combined: statistical: ARIMA, catboost: DWT_catboost, rf: rf, svr: FT_svr, naive: NaiveMovingAverage'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "df_viewer = df_agent[df_agent['dataset_index'] == ind]\n",
    "df_viewer['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Combined predictions from the best models in each category: Statistical (ARIMA), Naive (NaiveMovingAverage), RF (rf), SVR (FT_svr), and Catboost (DWT_catboost). The final result is an array of combined forecasts.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_a = df_a[df_a[\"final_test\"] == \"2024-11-30\"]\n",
    "df_a = df_a[df_a[\"dataset_index\"] == ind]\n",
    "df_a[\"description\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "mape     1867674109204215.2500\n",
      "pocid                  62.8372\n",
      "smape                   0.2304\n",
      "rmse                 6527.8739\n",
      "msmape                  0.2302\n",
      "mae                  5713.1712\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_agent = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_agent = df_agent[df_agent['final_test'] == '2024-11-30']\n",
    "\n",
    "print(len(df_agent))\n",
    "print(\n",
    "    df_agent[[\"mape\", \"pocid\", \"smape\", \"rmse\", \"msmape\", \"mae\"]]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "mape     2010481933651979.0000\n",
      "pocid                  62.7273\n",
      "smape                   0.2310\n",
      "rmse                 6471.4706\n",
      "msmape                  0.2308\n",
      "mae                  5652.2558\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_agent = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_agent = df_agent[df_agent['final_test'] == '2024-11-30']\n",
    "\n",
    "print(len(df_agent))\n",
    "print(\n",
    "    df_agent[[\"mape\", \"pocid\", \"smape\", \"rmse\", \"msmape\", \"mae\"]]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_best_model_by_rmse(models, dataset_index, final_test, val_test, base_path):\n",
    "    \"\"\"\n",
    "    Encontra o melhor modelo baseado em RMSE para um dataset específico.\n",
    "\n",
    "    Args:\n",
    "        models: Lista de nomes dos modelos\n",
    "        dataset_index: Índice do dataset\n",
    "        final_test: Data do teste final\n",
    "        base_path: Caminho base para os arquivos CSV\n",
    "\n",
    "    Returns:\n",
    "        Tupla (best_smape, best_model, best_predictions)\n",
    "    \"\"\"\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_val_model = \"\"\n",
    "    best_predictions = []\n",
    "    test = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{base_path}/{model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "            df = df[\n",
    "                (df[\"final_test\"] == val_test)\n",
    "                & (df[\"dataset_index\"] == dataset_index)\n",
    "            ]\n",
    "\n",
    "            if not df.empty:\n",
    "                rmse = df.iloc[0][\"smape\"]\n",
    "                if rmse < best_val_rmse:\n",
    "                    # print(f\"MODELO {best_val_model} é pior que {model}\")\n",
    "                    # print(f\"antigo: {best_val_rmse} | novo: {rmse}\")\n",
    "                    best_val_rmse = rmse\n",
    "                    best_val_model = model\n",
    "                    # best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "                    # test = extract_values(df.iloc[0][\"test\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar modelo {model}: {e}\")\n",
    "\n",
    "    df = pd.read_csv(f\"{base_path}/{best_val_model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "    df = df[(df[\"final_test\"] == final_test) & (df[\"dataset_index\"] == dataset_index)]\n",
    "    best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "    test = extract_values(df.iloc[0][\"test\"])\n",
    "\n",
    "    return best_val_rmse, best_val_model, best_predictions, test\n",
    "\n",
    "\n",
    "# Configurações\n",
    "base_path = \"/home/lucas/Documents/mestrado/Statistics_and_Seq2Seq/timeseries/mestrado/resultados\"\n",
    "\n",
    "# Definir todos os grupos de modelos\n",
    "model_groups = {\n",
    "    \"statistical\": [\"ARIMA\", \"ETS\", \"THETA\"],\n",
    "    \"catboost\": [\n",
    "        \"catboost\",\n",
    "        \"CWT_catboost\",\n",
    "        \"DWT_catboost\",\n",
    "        \"FT_catboost\",\n",
    "        \"ONLY_CWT_catboost\",\n",
    "        \"ONLY_DWT_catboost\",\n",
    "        \"ONLY_FT_catboost\",\n",
    "    ],\n",
    "    \"rf\": [\n",
    "        \"rf\",\n",
    "        \"CWT_rf\",\n",
    "        \"DWT_rf\",\n",
    "        \"FT_rf\",\n",
    "        \"ONLY_CWT_rf\",\n",
    "        \"ONLY_DWT_rf\",\n",
    "        \"ONLY_FT_rf\",\n",
    "    ],\n",
    "    \"svr\": [\n",
    "        \"svr\",\n",
    "        \"CWT_svr\",\n",
    "        \"DWT_svr\",\n",
    "        \"FT_svr\",\n",
    "        \"ONLY_CWT_svr\",\n",
    "        \"ONLY_DWT_svr\",\n",
    "        \"ONLY_FT_svr\",\n",
    "    ],\n",
    "    \"naive\": [\"NaiveSeasonal\", \"NaiveMovingAverage\"],\n",
    "}\n",
    "\n",
    "cols_serie = [\n",
    "    \"dataset_index\",\n",
    "    \"horizon\",\n",
    "    \"regressor\",\n",
    "    \"mape\",\n",
    "    \"pocid\",\n",
    "    \"smape\",\n",
    "    \"rmse\",\n",
    "    \"msmape\",\n",
    "    \"mae\",\n",
    "    \"test\",\n",
    "    \"predictions\",\n",
    "    \"start_test\",\n",
    "    \"final_test\",\n",
    "    \"description\",\n",
    "]\n",
    "\n",
    "# Processar todos os datasets\n",
    "combined_results = []\n",
    "\n",
    "import os\n",
    "from all_functions import *\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "dataset = \"ANP_MONTHLY\"\n",
    "exp_name = \"SMAPE_BEST_CATEGORY_EACH_SERIE\"\n",
    "path_experiments = f\"./timeseries/mestrado/resultados/{exp_name}/\"\n",
    "path_csv = f\"{path_experiments}/{dataset}.csv\"\n",
    "final_test = \"2024-11-30\"\n",
    "val_test = \"2023-11-30\"\n",
    "# os.makedirs(path_experiments, exist_ok=True)\n",
    "for i in range(0, 1):\n",
    "    best_results = {}\n",
    "    all_predictions = []\n",
    "\n",
    "    # Encontrar o melhor modelo de cada grupo\n",
    "    for group_name, models in model_groups.items():\n",
    "        val_rmse, model, preds, test = get_best_model_by_rmse(\n",
    "            models, i, final_test, val_test, base_path\n",
    "        )\n",
    "        print(\"------------------------\")\n",
    "        best_results[group_name] = {\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"model\": model,\n",
    "            \"predictions\": preds,\n",
    "        }\n",
    "\n",
    "        # Coletar predições válidas para combinar\n",
    "        if preds and len(preds) > 0:\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    # Combinar predições por média\n",
    "    if all_predictions:\n",
    "        # Converter para array numpy para facilitar a média\n",
    "        predictions_array = np.array(all_predictions)\n",
    "        combined_prediction = np.mean(predictions_array, axis=0).tolist()\n",
    "    else:\n",
    "        combined_prediction = []\n",
    "\n",
    "    # Encontrar o melhor entre todos os grupos\n",
    "    overall_best_group = min(best_results.items(), key=lambda x: x[1][\"val_rmse\"])\n",
    "\n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    for group_name, result in best_results.items():\n",
    "        if result[\"model\"]:\n",
    "            print(\n",
    "                f\"  Melhor {group_name}: {result['model']} (RMSE na validação: {result['val_rmse']:.4f})\"\n",
    "            )\n",
    "\n",
    "    preds_real = combined_prediction\n",
    "\n",
    "    test = np.array(test)\n",
    "    preds_real_array = np.array(preds_real)\n",
    "    preds_real_reshaped = preds_real_array.reshape(1, -1)\n",
    "    test_reshaped = test.reshape(1, -1)\n",
    "    smape_result = calculate_smape(preds_real_reshaped, test_reshaped)\n",
    "    # print(smape_result)\n",
    "    rmse_result = calculate_rmse(preds_real_reshaped, test_reshaped)\n",
    "    msmape_result = calculate_msmape(preds_real_reshaped, test_reshaped)\n",
    "    # mase_result = calculate_mase(preds_real_reshaped, test_reshaped, training_set, seasonality)\n",
    "    mae_result = calculate_mae(preds_real_reshaped, test_reshaped)\n",
    "    mape_result = mape(test, preds_real_array)\n",
    "    pocid_result = pocid(test, preds_real_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_best_model_by_rmse(models, dataset_index, final_test, val_test, base_path):\n",
    "    \"\"\"\n",
    "    Encontra o melhor modelo baseado em RMSE para um dataset específico.\n",
    "\n",
    "    Args:\n",
    "        models: Lista de nomes dos modelos\n",
    "        dataset_index: Índice do dataset\n",
    "        final_test: Data do teste final\n",
    "        base_path: Caminho base para os arquivos CSV\n",
    "\n",
    "    Returns:\n",
    "        Tupla (best_smape, best_model, best_predictions)\n",
    "    \"\"\"\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_val_model = \"\"\n",
    "    best_predictions = []\n",
    "    test = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{base_path}/{model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "            df = df[\n",
    "                (df[\"final_test\"] == val_test) & (df[\"dataset_index\"] == dataset_index)\n",
    "            ]\n",
    "\n",
    "            if not df.empty:\n",
    "                rmse = df.iloc[0][\"smape\"]\n",
    "                if rmse < best_val_rmse:\n",
    "                    # print(f\"MODELO {best_val_model} é pior que {model}\")\n",
    "                    # print(f\"antigo: {best_val_rmse} | novo: {rmse}\")\n",
    "                    best_val_rmse = rmse\n",
    "                    best_val_model = model\n",
    "                    # best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "                    # test = extract_values(df.iloc[0][\"test\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar modelo {model}: {e}\")\n",
    "\n",
    "    df = pd.read_csv(f\"{base_path}/{best_val_model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "    df = df[(df[\"final_test\"] == final_test) & (df[\"dataset_index\"] == dataset_index)]\n",
    "    best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "    test = extract_values(df.iloc[0][\"test\"])\n",
    "\n",
    "    return best_val_rmse, best_val_model, best_predictions, test\n",
    "\n",
    "\n",
    "# Configurações\n",
    "base_path = \"/home/lucas/Documents/mestrado/Statistics_and_Seq2Seq/timeseries/mestrado/resultados\"\n",
    "final_test = \"2024-11-30\"\n",
    "\n",
    "# Definir todos os grupos de modelos\n",
    "model_groups = {\n",
    "    \"statistical\": [\"ARIMA\", \"ETS\", \"THETA\"],\n",
    "    \"catboost\": [\"catboost\", \"CWT_catboost\", \"DWT_catboost\", \"FT_catboost\", \n",
    "                 \"ONLY_CWT_catboost\", \"ONLY_DWT_catboost\", \"ONLY_FT_catboost\"],\n",
    "    \"rf\": [\"rf\", \"CWT_rf\", \"DWT_rf\", \"FT_rf\", \"ONLY_CWT_rf\", \"ONLY_DWT_rf\", \"ONLY_FT_rf\"],\n",
    "    \"svr\": [\"svr\", \"CWT_svr\", \"DWT_svr\", \"FT_svr\", \"ONLY_CWT_svr\", \"ONLY_DWT_svr\", \"ONLY_FT_svr\"],\n",
    "    \"naive\": [\"NaiveSeasonal\", \"NaiveMovingAverage\"]\n",
    "}\n",
    "\n",
    "cols_serie = [\n",
    "    \"dataset_index\",\n",
    "    \"horizon\",\n",
    "    \"regressor\",\n",
    "    \"mape\",\n",
    "    \"pocid\",\n",
    "    \"smape\",\n",
    "    \"rmse\",\n",
    "    \"msmape\",\n",
    "    \"mae\",\n",
    "    \"test\",\n",
    "    \"predictions\",\n",
    "    \"start_test\",\n",
    "    \"final_test\",\n",
    "    \"description\",\n",
    "]\n",
    "\n",
    "# Processar todos os datasets\n",
    "combined_results = []\n",
    "\n",
    "import os\n",
    "from all_functions import *\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "dataset = \"ANP_MONTHLY\"\n",
    "exp_name = \"SMAPE_BEST_CATEGORY_EACH_SERIE\"\n",
    "path_experiments = (\n",
    "        f\"./timeseries/mestrado/resultados/{exp_name}/\"\n",
    "    )\n",
    "path_csv = f\"{path_experiments}/{dataset}.csv\"\n",
    "os.makedirs(path_experiments, exist_ok=True)\n",
    "val_test = \"2023-11-30\"\n",
    "for i in range(0, 182):\n",
    "    best_results = {}\n",
    "    all_predictions = []\n",
    "\n",
    "    # Encontrar o melhor modelo de cada grupo\n",
    "    for group_name, models in model_groups.items():\n",
    "        val_rmse, model, preds, test = get_best_model_by_rmse(models, i, final_test,val_test, base_path)\n",
    "        best_results[group_name] = {\n",
    "            'val_rmse': val_rmse,\n",
    "            'model': model,\n",
    "            'predictions': preds\n",
    "        }\n",
    "\n",
    "        # Coletar predições válidas para combinar\n",
    "        if preds and len(preds) > 0:\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    # Combinar predições por média\n",
    "    if all_predictions:\n",
    "        # Converter para array numpy para facilitar a média\n",
    "        predictions_array = np.array(all_predictions)\n",
    "        combined_prediction = np.mean(predictions_array, axis=0).tolist()\n",
    "    else:\n",
    "        combined_prediction = []\n",
    "\n",
    "    # Encontrar o melhor entre todos os grupos\n",
    "    overall_best_group = min(best_results.items(), key=lambda x: x[1]['val_rmse'])\n",
    "\n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    print(combined_prediction)\n",
    "    # Mostrar os melhores de cada grupo\n",
    "    for group_name, result in best_results.items():\n",
    "        if result['model']:\n",
    "            print(f\"  Melhor {group_name}: {result['model']} (SMAPE: {result['val_rmse']:.4f})\")\n",
    "            \n",
    "    \n",
    "    preds_real = combined_prediction\n",
    "    \n",
    "    test = np.array(test)\n",
    "    preds_real_array = np.array(preds_real)\n",
    "    preds_real_reshaped = preds_real_array.reshape(1, -1)\n",
    "    test_reshaped = test.reshape(1, -1)\n",
    "    smape_result = calculate_smape(preds_real_reshaped, test_reshaped)\n",
    "    # print(smape_result)\n",
    "    rmse_result = calculate_rmse(preds_real_reshaped, test_reshaped)\n",
    "    msmape_result = calculate_msmape(preds_real_reshaped, test_reshaped)\n",
    "    # mase_result = calculate_mase(preds_real_reshaped, test_reshaped, training_set, seasonality)\n",
    "    mae_result = calculate_mae(preds_real_reshaped, test_reshaped)\n",
    "    mape_result = mape(test, preds_real_array)\n",
    "    pocid_result = pocid(test, preds_real_array)\n",
    "    \n",
    "    description = \"Models combined: \" + \", \".join([f\"{group}: {best_results[group]['model']}\" for group in best_results if best_results[group]['model']])\n",
    "    data_serie = {\n",
    "            \"dataset_index\": f\"{i}\",\n",
    "            \"horizon\": \"12\",\n",
    "            \"regressor\": \"BEST_CATEGORY_EACH_SERIE\",\n",
    "            \"mape\": mape_result,\n",
    "            \"pocid\": pocid_result,\n",
    "            \"smape\": smape_result,\n",
    "            \"rmse\": rmse_result,\n",
    "            \"msmape\": msmape_result,\n",
    "            \"mae\": mae_result,\n",
    "            \"test\": [test.tolist()],\n",
    "            \"predictions\": [preds_real],\n",
    "            \"start_test\": \"INICIO\",\n",
    "            \"final_test\": final_test,\n",
    "            \"description\": description,\n",
    "            # 'training_time': times[0],\n",
    "            # 'prediction_time': times[1],\n",
    "        }\n",
    "\n",
    "    if not os.path.exists(path_csv):\n",
    "            pd.DataFrame(columns=cols_serie).to_csv(path_csv, sep=\";\", index=False)\n",
    "\n",
    "    print(\"Salvando resultados...\\n\")\n",
    "    df_new = pd.DataFrame(data_serie)\n",
    "    df_new.to_csv(path_csv, sep=\";\", mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "    # print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def _ensure_array(y: Union[float, List[float]], n: int) -> np.ndarray:\n",
    "    \"\"\"Garante que y tem tamanho n; se for escalar, repete para formar array de tamanho n.\"\"\"\n",
    "    if isinstance(y, (list, tuple, np.ndarray)):\n",
    "        y_arr = np.asarray(y, dtype=float)\n",
    "        if y_arr.shape[0] != n:\n",
    "            raise ValueError(\n",
    "                f\"Lista y_val tem tamanho {y_arr.shape[0]} mas esperava {n}\"\n",
    "            )\n",
    "        return y_arr\n",
    "    else:\n",
    "        return np.full(shape=(n,), fill_value=float(y), dtype=float)\n",
    "\n",
    "\n",
    "def check_input_shapes(\n",
    "    val_preds: Dict[str, List[float]], test_preds: Dict[str, List[float]]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Verifica consistência mínima:\n",
    "    - todas as listas em val_preds têm o mesmo tamanho n_val\n",
    "    - todos os modelos em val_preds existem em test_preds (test_preds pode ter listas de tamanho diferente)\n",
    "    Retorna n_val.\n",
    "    \"\"\"\n",
    "    if not val_preds:\n",
    "        raise ValueError(\"val_preds está vazio\")\n",
    "    lengths = {k: len(v) for k, v in val_preds.items()}\n",
    "    n_vals = set(lengths.values())\n",
    "    if len(n_vals) != 1:\n",
    "        raise ValueError(f\"Tamanhos inconsistentes nas listas de validação: {lengths}\")\n",
    "    n_val = n_vals.pop()\n",
    "\n",
    "    # checar test_preds contém as chaves\n",
    "    for k in val_preds:\n",
    "        if k not in test_preds:\n",
    "            raise ValueError(\n",
    "                f\"Modelo '{k}' presente em val_preds mas ausente em test_preds\"\n",
    "            )\n",
    "        if not isinstance(test_preds[k], (list, tuple, np.ndarray)):\n",
    "            raise ValueError(f\"test_preds['{k}'] precisa ser lista/np.array\")\n",
    "    return n_val\n",
    "\n",
    "\n",
    "def shift_correction(\n",
    "    val_preds: Dict[str, List[float]],\n",
    "    y_val: List[float],\n",
    "    test_preds: Dict[str, List[float]],\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Corrige previsões adicionando um bias por modelo:\n",
    "      bias = mean(y_val) - mean(pred_val_model)\n",
    "    Aplica bias às previsões de teste e retorna dicionário de previsões corrigidas.\n",
    "    \"\"\"\n",
    "    n_val = check_input_shapes(val_preds, test_preds)\n",
    "    y_val_arr = _ensure_array(y_val, n_val)\n",
    "    y_mean = float(np.mean(y_val_arr))\n",
    "\n",
    "    corrected = {}\n",
    "    for name, preds_val in val_preds.items():\n",
    "        preds_val_arr = np.asarray(preds_val, dtype=float)\n",
    "        bias = y_mean - float(np.mean(preds_val_arr))\n",
    "        preds_test_arr = np.asarray(test_preds[name], dtype=float)\n",
    "        corrected[name] = (preds_test_arr + bias).tolist()\n",
    "\n",
    "    return corrected\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = {\n",
    "    \"ARIMA\": [\n",
    "        400.71678399,\n",
    "        663.87332953,\n",
    "        929.11779741,\n",
    "        928.19414546,\n",
    "        820.45108076,\n",
    "        942.73181293,\n",
    "        917.04772083,\n",
    "        777.88168678,\n",
    "        643.23803468,\n",
    "        743.95702414,\n",
    "        786.77982244,\n",
    "        730.3459295,\n",
    "    ],\n",
    "    \"FT_catboost\": [\n",
    "        400.71678399,\n",
    "        663.87332953,\n",
    "        929.11779741,\n",
    "        928.19414546,\n",
    "        820.45108076,\n",
    "        942.73181293,\n",
    "        917.04772083,\n",
    "        777.88168678,\n",
    "        643.23803468,\n",
    "        743.95702414,\n",
    "        786.77982244,\n",
    "        730.3459295,\n",
    "    ],\n",
    "    \"ONLY_DWT_rf\": [\n",
    "        355.62580122,\n",
    "        450.92858957,\n",
    "        417.68229707,\n",
    "        1332.2369491,\n",
    "        529.15589532,\n",
    "        921.31116252,\n",
    "        974.07278312,\n",
    "        809.75277656,\n",
    "        747.63653457,\n",
    "        549.20459147,\n",
    "        1694.72720717,\n",
    "        1188.96124668,\n",
    "    ],\n",
    "    \"FT_svr\": [\n",
    "        480.87089365,\n",
    "        520.23308461,\n",
    "        632.27912708,\n",
    "        657.58230609,\n",
    "        471.22153901,\n",
    "        522.99862274,\n",
    "        496.83943868,\n",
    "        477.60744965,\n",
    "        398.98449469,\n",
    "        337.1191924,\n",
    "        405.94991964,\n",
    "        352.95463704,\n",
    "    ],\n",
    "    \"NaiveSeasonal\": [\n",
    "        163.35,\n",
    "        124.32,\n",
    "        1767.006,\n",
    "        1003.093,\n",
    "        76.45,\n",
    "        2023.998,\n",
    "        2251.496,\n",
    "        1055.33,\n",
    "        245.87,\n",
    "        162.73,\n",
    "        670.983,\n",
    "        -1.13686838e-13,\n",
    "    ],\n",
    "}\n",
    "y_val = [\n",
    "    47.51,\n",
    "    0.0,\n",
    "    62.15,\n",
    "    4214.105,\n",
    "    92.68,\n",
    "    1030.353,\n",
    "    2585.757,\n",
    "    1267.991,\n",
    "    0.0,\n",
    "    2517.924,\n",
    "    855.162,\n",
    "    250.91,\n",
    "]\n",
    "\n",
    "test_preds = {\n",
    "    \"ARIMA\": [\n",
    "        626.54445435,\n",
    "        867.25753704,\n",
    "        1140.44178255,\n",
    "        1218.40255972,\n",
    "        922.16557401,\n",
    "        1025.36568259,\n",
    "        1277.64676113,\n",
    "        1043.62573386,\n",
    "        871.6622963,\n",
    "        977.16178989,\n",
    "        938.74572533,\n",
    "        920.28328892,\n",
    "    ],\n",
    "    \"FT_catboost\": [\n",
    "        287.72484798,\n",
    "        -296.80893321,\n",
    "        259.47301569,\n",
    "        -221.27725565,\n",
    "        255.15551412,\n",
    "        254.38388207,\n",
    "        203.31202369,\n",
    "        793.22870178,\n",
    "        1008.24206288,\n",
    "        1084.23653574,\n",
    "        554.45271123,\n",
    "        564.5496957,\n",
    "    ],\n",
    "    \"ONLY_DWT_rf\": [\n",
    "        1167.69032389,\n",
    "        718.71243323,\n",
    "        352.10670718,\n",
    "        844.43235289,\n",
    "        2291.78410799,\n",
    "        889.53650482,\n",
    "        993.63343095,\n",
    "        1258.16041135,\n",
    "        595.12303392,\n",
    "        1212.79863338,\n",
    "        1000.14051588,\n",
    "        2113.93667613,\n",
    "    ],\n",
    "    \"FT_svr\": [\n",
    "        -387.22267965,\n",
    "        -1602.8661116,\n",
    "        -1434.76910943,\n",
    "        -1765.4953112,\n",
    "        -561.58789508,\n",
    "        353.94495089,\n",
    "        816.7784578,\n",
    "        971.05251116,\n",
    "        649.41751243,\n",
    "        1055.17371582,\n",
    "        282.25494992,\n",
    "        149.97195324,\n",
    "    ],\n",
    "    \"NaiveSeasonal\": [\n",
    "        47.51,\n",
    "        -2.27373675e-13,\n",
    "        62.15,\n",
    "        4214.105,\n",
    "        92.68,\n",
    "        1030.353,\n",
    "        2585.757,\n",
    "        1267.991,\n",
    "        -2.27373675e-13,\n",
    "        2517.924,\n",
    "        855.162,\n",
    "        250.91,\n",
    "    ],\n",
    "}\n",
    "y_test = [2101.917,\n",
    " 4194.392,\n",
    " 4443.881,\n",
    " 3153.49,\n",
    " 1398.42,\n",
    " 680.292,\n",
    " -3.036,\n",
    " 2000.117,\n",
    " 2160.356,\n",
    " 1995.957,\n",
    " 4128.637,\n",
    " 4555.324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.51127291866553"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "new_preds = linear_correction(val_preds=val_preds, y_val=y_val, test_preds=test_preds)\n",
    "\n",
    "preds_list = [preds for preds in new_preds.values()]\n",
    "combined = np.mean(preds_list, axis=0)\n",
    "mape(y_test, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[673.091,\n",
       " 645.903,\n",
       " 2587.536,\n",
       " 1970.165,\n",
       " 3338.866,\n",
       " 642.657,\n",
       " 328.47,\n",
       " 290.66,\n",
       " 729.046,\n",
       " 654.321,\n",
       " 1048.09,\n",
       " 719.027]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "index = 12\n",
    "df_arima = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/NaiveSeasonal/normal/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_arima = df_arima[df_arima[\"final_test\"] == \"2020-11-30\"]\n",
    "df_arima = df_arima[df_arima[\"dataset_index\"] == index]\n",
    "extract_values(df_arima['test'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[626.54445435,\n",
       " 867.25753704,\n",
       " 1140.44178255,\n",
       " 1218.40255972,\n",
       " 922.16557401,\n",
       " 1025.36568259,\n",
       " 1277.64676113,\n",
       " 1043.62573386,\n",
       " 871.6622963,\n",
       " 977.16178989,\n",
       " 938.74572533,\n",
       " 920.28328892]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index = 12\n",
    "df_arima = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/ARIMA/normal/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_arima = df_arima[df_arima[\"final_test\"] == \"2024-11-30\"]\n",
    "df_arima = df_arima[df_arima[\"dataset_index\"] == index]\n",
    "extract_values(df_arima[\"predictions\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
