{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANP_MONTHLY.csv: 5642 entries\n",
      "AUSTRALIAN_ELECTRICITY_DEMAND_DATASET.csv: 20 entries\n",
      "M4_HOURLY_DATASET.csv: 1656 entries\n",
      "M4_WEEKLY_DATASET.csv: 1436 entries\n",
      "NN5_DAILY_DATASET_WITHOUT_MISSING_VALUES.csv: 444 entries\n",
      "NN5_WEEKLY_DATASET.csv: 444 entries\n",
      "Erro ao ler ./timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTh1.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTh1.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTh2.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTh2.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTm1.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTm1.csv'\n",
      "Erro ao ler ./timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTm2.csv: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/ONLY_DWT_rf/normal/ETTm2.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "\n",
    "model = \"ONLY_DWT_rf\"\n",
    "part = '/normal'\n",
    "csv_files = [\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ANP_MONTHLY.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/AUSTRALIAN_ELECTRICITY_DEMAND_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/M4_HOURLY_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/M4_WEEKLY_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/NN5_DAILY_DATASET_WITHOUT_MISSING_VALUES.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/NN5_WEEKLY_DATASET.csv\",\n",
    "    # f\"./timeseries/mestrado/resultados/{model}{part}/PEDESTRIAN_COUNTS_DATASET.csv\",\n",
    "    # f\"./timeseries/mestrado/resultados/{model}{part}/US_BIRTHS_DATASET.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTh1.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTh2.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTm1.csv\",\n",
    "    f\"./timeseries/mestrado/resultados/{model}{part}/ETTm2.csv\",\n",
    "]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, sep=\";\")\n",
    "        print(f\"{csv_file.split('/')[-1]}: {len(df)} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler {csv_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_index</th>\n",
       "      <th>horizon</th>\n",
       "      <th>regressor</th>\n",
       "      <th>mape</th>\n",
       "      <th>pocid</th>\n",
       "      <th>smape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>msmape</th>\n",
       "      <th>mae</th>\n",
       "      <th>test</th>\n",
       "      <th>predictions</th>\n",
       "      <th>start_test</th>\n",
       "      <th>final_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>catboost_normal</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>95.7447</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>50.9475</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>36.1896</td>\n",
       "      <td>[619.0, 565.0, 532.0, 495.0, 481.0, 467.0, 473...</td>\n",
       "      <td>[630.15559853 573.21928858 529.48314001 498.36...</td>\n",
       "      <td>2015-07-30 16:00:00</td>\n",
       "      <td>2015-08-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>catboost_normal</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>93.6170</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>59.3014</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>50.6220</td>\n",
       "      <td>[664.0, 550.0, 544.0, 505.0, 483.0, 469.0, 466...</td>\n",
       "      <td>[662.04055879 578.0305773  527.01390442 481.01...</td>\n",
       "      <td>2015-07-28 16:00:00</td>\n",
       "      <td>2015-07-30 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>catboost_normal</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>87.2340</td>\n",
       "      <td>0.0624</td>\n",
       "      <td>60.3221</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>44.0808</td>\n",
       "      <td>[622.0, 558.0, 513.0, 476.0, 449.0, 437.0, 422...</td>\n",
       "      <td>[631.06387725 572.18403956 516.36529877 479.78...</td>\n",
       "      <td>2015-07-26 16:00:00</td>\n",
       "      <td>2015-07-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>catboost_normal</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>85.1064</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>39.5394</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>32.7369</td>\n",
       "      <td>[598.0, 547.0, 503.0, 474.0, 459.0, 450.0, 450...</td>\n",
       "      <td>[603.18721866 541.36052344 517.14223334 497.14...</td>\n",
       "      <td>2015-07-24 16:00:00</td>\n",
       "      <td>2015-07-26 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_index  horizon        regressor   mape   pocid  smape    rmse  \\\n",
       "0               0       48  catboost_normal 0.0520 95.7447 0.0498 50.9475   \n",
       "8               0       48  catboost_normal 0.0772 93.6170 0.0817 59.3014   \n",
       "15              0       48  catboost_normal 0.0600 87.2340 0.0624 60.3221   \n",
       "20              0       48  catboost_normal 0.0583 85.1064 0.0557 39.5394   \n",
       "\n",
       "    msmape     mae                                               test  \\\n",
       "0   0.0498 36.1896  [619.0, 565.0, 532.0, 495.0, 481.0, 467.0, 473...   \n",
       "8   0.0817 50.6220  [664.0, 550.0, 544.0, 505.0, 483.0, 469.0, 466...   \n",
       "15  0.0623 44.0808  [622.0, 558.0, 513.0, 476.0, 449.0, 437.0, 422...   \n",
       "20  0.0557 32.7369  [598.0, 547.0, 503.0, 474.0, 459.0, 450.0, 450...   \n",
       "\n",
       "                                          predictions           start_test  \\\n",
       "0   [630.15559853 573.21928858 529.48314001 498.36...  2015-07-30 16:00:00   \n",
       "8   [662.04055879 578.0305773  527.01390442 481.01...  2015-07-28 16:00:00   \n",
       "15  [631.06387725 572.18403956 516.36529877 479.78...  2015-07-26 16:00:00   \n",
       "20  [603.18721866 541.36052344 517.14223334 497.14...  2015-07-24 16:00:00   \n",
       "\n",
       "             final_test  \n",
       "0   2015-08-01 15:00:00  \n",
       "8   2015-07-30 15:00:00  \n",
       "15  2015-07-28 15:00:00  \n",
       "20  2015-07-26 15:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "df_agent = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/catboost/normal/M4_HOURLY_DATASET.csv\",\n",
    "    sep=\";\",\n",
    "    )\n",
    "\n",
    "df_agent[df_agent['dataset_index'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Models combined: statistical: ARIMA, catboost: DWT_catboost, rf: rf, svr: FT_svr, naive: NaiveMovingAverage'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "df_viewer = df_agent[df_agent['dataset_index'] == ind]\n",
    "df_viewer['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_a = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\u001b[39;00m\n\u001b[32m      8\u001b[39m df_a = df_a[df_a[\u001b[33m\"\u001b[39m\u001b[33mfinal_test\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33m2024-11-30\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agno/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    899\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    900\u001b[39m     dialect,\n\u001b[32m    901\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    908\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    909\u001b[39m )\n\u001b[32m    910\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agno/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    574\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agno/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1406\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1407\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agno/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1659\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1660\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1672\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agno/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    856\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    858\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    867\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    868\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/simple_selective_agent_qwen3=14b/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_a = df_a[df_a[\"final_test\"] == \"2024-11-30\"]\n",
    "df_a = df_a[df_a[\"dataset_index\"] == ind]\n",
    "df_a[\"description\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "mape     1580401912908547.7500\n",
      "pocid                  60.3896\n",
      "smape                   0.2255\n",
      "rmse                 6787.3698\n",
      "msmape                  0.2253\n",
      "mae                  5917.4616\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_agent = pd.read_csv(\n",
    "    \"/home/anp/Documents/lucas_mestrado/Statistics_and_Seq2Seq/Statistics_and_Seq2Seq/timeseries/mestrado/resultados/orchestrator_deterministic_v1/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_agent = df_agent[df_agent['final_test'] == '2024-11-30']\n",
    "\n",
    "print(len(df_agent))\n",
    "print(\n",
    "    df_agent[[\"mape\", \"pocid\", \"smape\", \"rmse\", \"msmape\", \"mae\"]]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "mape     1867671054740872.7500\n",
      "pocid                  62.8871\n",
      "smape                   0.2305\n",
      "rmse                 6529.2548\n",
      "msmape                  0.2304\n",
      "mae                  5715.5957\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_agent = pd.read_csv(\n",
    "    \"./timeseries/mestrado/final/BEST_CATEGORY_EACH_SERIE/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_agent = df_agent[df_agent['final_test'] == '2024-11-30']\n",
    "\n",
    "print(len(df_agent))\n",
    "print(\n",
    "    df_agent[[\"mape\", \"pocid\", \"smape\", \"rmse\", \"msmape\", \"mae\"]]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_best_model_by_rmse(models, dataset_index, final_test, val_test, base_path):\n",
    "    \"\"\"\n",
    "    Encontra o melhor modelo baseado em RMSE para um dataset específico.\n",
    "\n",
    "    Args:\n",
    "        models: Lista de nomes dos modelos\n",
    "        dataset_index: Índice do dataset\n",
    "        final_test: Data do teste final\n",
    "        base_path: Caminho base para os arquivos CSV\n",
    "\n",
    "    Returns:\n",
    "        Tupla (best_smape, best_model, best_predictions)\n",
    "    \"\"\"\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_val_model = \"\"\n",
    "    best_predictions = []\n",
    "    test = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{base_path}/{model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "            df = df[\n",
    "                (df[\"final_test\"] == val_test)\n",
    "                & (df[\"dataset_index\"] == dataset_index)\n",
    "            ]\n",
    "\n",
    "            if not df.empty:\n",
    "                rmse = df.iloc[0][\"smape\"]\n",
    "                if rmse < best_val_rmse:\n",
    "                    # print(f\"MODELO {best_val_model} é pior que {model}\")\n",
    "                    # print(f\"antigo: {best_val_rmse} | novo: {rmse}\")\n",
    "                    best_val_rmse = rmse\n",
    "                    best_val_model = model\n",
    "                    # best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "                    # test = extract_values(df.iloc[0][\"test\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar modelo {model}: {e}\")\n",
    "\n",
    "    df = pd.read_csv(f\"{base_path}/{best_val_model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "    df = df[(df[\"final_test\"] == final_test) & (df[\"dataset_index\"] == dataset_index)]\n",
    "    best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "    test = extract_values(df.iloc[0][\"test\"])\n",
    "\n",
    "    return best_val_rmse, best_val_model, best_predictions, test\n",
    "\n",
    "\n",
    "# Configurações\n",
    "base_path = \"/home/lucas/Documents/mestrado/Statistics_and_Seq2Seq/timeseries/mestrado/resultados\"\n",
    "\n",
    "# Definir todos os grupos de modelos\n",
    "model_groups = {\n",
    "    \"statistical\": [\"ARIMA\", \"ETS\", \"THETA\"],\n",
    "    \"catboost\": [\n",
    "        \"catboost\",\n",
    "        \"CWT_catboost\",\n",
    "        \"DWT_catboost\",\n",
    "        \"FT_catboost\",\n",
    "        \"ONLY_CWT_catboost\",\n",
    "        \"ONLY_DWT_catboost\",\n",
    "        \"ONLY_FT_catboost\",\n",
    "    ],\n",
    "    \"rf\": [\n",
    "        \"rf\",\n",
    "        \"CWT_rf\",\n",
    "        \"DWT_rf\",\n",
    "        \"FT_rf\",\n",
    "        \"ONLY_CWT_rf\",\n",
    "        \"ONLY_DWT_rf\",\n",
    "        \"ONLY_FT_rf\",\n",
    "    ],\n",
    "    \"svr\": [\n",
    "        \"svr\",\n",
    "        \"CWT_svr\",\n",
    "        \"DWT_svr\",\n",
    "        \"FT_svr\",\n",
    "        \"ONLY_CWT_svr\",\n",
    "        \"ONLY_DWT_svr\",\n",
    "        \"ONLY_FT_svr\",\n",
    "    ],\n",
    "    \"naive\": [\"NaiveSeasonal\", \"NaiveMovingAverage\"],\n",
    "}\n",
    "\n",
    "cols_serie = [\n",
    "    \"dataset_index\",\n",
    "    \"horizon\",\n",
    "    \"regressor\",\n",
    "    \"mape\",\n",
    "    \"pocid\",\n",
    "    \"smape\",\n",
    "    \"rmse\",\n",
    "    \"msmape\",\n",
    "    \"mae\",\n",
    "    \"test\",\n",
    "    \"predictions\",\n",
    "    \"start_test\",\n",
    "    \"final_test\",\n",
    "    \"description\",\n",
    "]\n",
    "\n",
    "# Processar todos os datasets\n",
    "combined_results = []\n",
    "\n",
    "import os\n",
    "from all_functions import *\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "dataset = \"ANP_MONTHLY\"\n",
    "exp_name = \"SMAPE_BEST_CATEGORY_EACH_SERIE\"\n",
    "path_experiments = f\"./timeseries/mestrado/resultados/{exp_name}/\"\n",
    "path_csv = f\"{path_experiments}/{dataset}.csv\"\n",
    "final_test = \"2024-11-30\"\n",
    "val_test = \"2023-11-30\"\n",
    "# os.makedirs(path_experiments, exist_ok=True)\n",
    "for i in range(0, 1):\n",
    "    best_results = {}\n",
    "    all_predictions = []\n",
    "\n",
    "    # Encontrar o melhor modelo de cada grupo\n",
    "    for group_name, models in model_groups.items():\n",
    "        val_rmse, model, preds, test = get_best_model_by_rmse(\n",
    "            models, i, final_test, val_test, base_path\n",
    "        )\n",
    "        print(\"------------------------\")\n",
    "        best_results[group_name] = {\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"model\": model,\n",
    "            \"predictions\": preds,\n",
    "        }\n",
    "\n",
    "        # Coletar predições válidas para combinar\n",
    "        if preds and len(preds) > 0:\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    # Combinar predições por média\n",
    "    if all_predictions:\n",
    "        # Converter para array numpy para facilitar a média\n",
    "        predictions_array = np.array(all_predictions)\n",
    "        combined_prediction = np.mean(predictions_array, axis=0).tolist()\n",
    "    else:\n",
    "        combined_prediction = []\n",
    "\n",
    "    # Encontrar o melhor entre todos os grupos\n",
    "    overall_best_group = min(best_results.items(), key=lambda x: x[1][\"val_rmse\"])\n",
    "\n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    for group_name, result in best_results.items():\n",
    "        if result[\"model\"]:\n",
    "            print(\n",
    "                f\"  Melhor {group_name}: {result['model']} (RMSE na validação: {result['val_rmse']:.4f})\"\n",
    "            )\n",
    "\n",
    "    preds_real = combined_prediction\n",
    "\n",
    "    test = np.array(test)\n",
    "    preds_real_array = np.array(preds_real)\n",
    "    preds_real_reshaped = preds_real_array.reshape(1, -1)\n",
    "    test_reshaped = test.reshape(1, -1)\n",
    "    smape_result = calculate_smape(preds_real_reshaped, test_reshaped)\n",
    "    # print(smape_result)\n",
    "    rmse_result = calculate_rmse(preds_real_reshaped, test_reshaped)\n",
    "    msmape_result = calculate_msmape(preds_real_reshaped, test_reshaped)\n",
    "    # mase_result = calculate_mase(preds_real_reshaped, test_reshaped, training_set, seasonality)\n",
    "    mae_result = calculate_mae(preds_real_reshaped, test_reshaped)\n",
    "    mape_result = mape(test, preds_real_array)\n",
    "    pocid_result = pocid(test, preds_real_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_best_model_by_rmse(models, dataset_index, final_test, val_test, base_path):\n",
    "    \"\"\"\n",
    "    Encontra o melhor modelo baseado em RMSE para um dataset específico.\n",
    "\n",
    "    Args:\n",
    "        models: Lista de nomes dos modelos\n",
    "        dataset_index: Índice do dataset\n",
    "        final_test: Data do teste final\n",
    "        base_path: Caminho base para os arquivos CSV\n",
    "\n",
    "    Returns:\n",
    "        Tupla (best_smape, best_model, best_predictions)\n",
    "    \"\"\"\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_val_model = \"\"\n",
    "    best_predictions = []\n",
    "    test = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{base_path}/{model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "            df = df[\n",
    "                (df[\"final_test\"] == val_test) & (df[\"dataset_index\"] == dataset_index)\n",
    "            ]\n",
    "\n",
    "            if not df.empty:\n",
    "                rmse = df.iloc[0][\"smape\"]\n",
    "                if rmse < best_val_rmse:\n",
    "                    # print(f\"MODELO {best_val_model} é pior que {model}\")\n",
    "                    # print(f\"antigo: {best_val_rmse} | novo: {rmse}\")\n",
    "                    best_val_rmse = rmse\n",
    "                    best_val_model = model\n",
    "                    # best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "                    # test = extract_values(df.iloc[0][\"test\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar modelo {model}: {e}\")\n",
    "\n",
    "    df = pd.read_csv(f\"{base_path}/{best_val_model}/normal/ANP_MONTHLY.csv\", sep=\";\")\n",
    "    df = df[(df[\"final_test\"] == final_test) & (df[\"dataset_index\"] == dataset_index)]\n",
    "    best_predictions = extract_values(df.iloc[0][\"predictions\"])\n",
    "    test = extract_values(df.iloc[0][\"test\"])\n",
    "\n",
    "    return best_val_rmse, best_val_model, best_predictions, test\n",
    "\n",
    "\n",
    "# Configurações\n",
    "base_path = \"/home/lucas/Documents/mestrado/Statistics_and_Seq2Seq/timeseries/mestrado/resultados\"\n",
    "final_test = \"2024-11-30\"\n",
    "\n",
    "# Definir todos os grupos de modelos\n",
    "model_groups = {\n",
    "    \"statistical\": [\"ARIMA\", \"ETS\", \"THETA\"],\n",
    "    \"catboost\": [\"catboost\", \"CWT_catboost\", \"DWT_catboost\", \"FT_catboost\", \n",
    "                 \"ONLY_CWT_catboost\", \"ONLY_DWT_catboost\", \"ONLY_FT_catboost\"],\n",
    "    \"rf\": [\"rf\", \"CWT_rf\", \"DWT_rf\", \"FT_rf\", \"ONLY_CWT_rf\", \"ONLY_DWT_rf\", \"ONLY_FT_rf\"],\n",
    "    \"svr\": [\"svr\", \"CWT_svr\", \"DWT_svr\", \"FT_svr\", \"ONLY_CWT_svr\", \"ONLY_DWT_svr\", \"ONLY_FT_svr\"],\n",
    "    \"naive\": [\"NaiveSeasonal\", \"NaiveMovingAverage\"]\n",
    "}\n",
    "\n",
    "cols_serie = [\n",
    "    \"dataset_index\",\n",
    "    \"horizon\",\n",
    "    \"regressor\",\n",
    "    \"mape\",\n",
    "    \"pocid\",\n",
    "    \"smape\",\n",
    "    \"rmse\",\n",
    "    \"msmape\",\n",
    "    \"mae\",\n",
    "    \"test\",\n",
    "    \"predictions\",\n",
    "    \"start_test\",\n",
    "    \"final_test\",\n",
    "    \"description\",\n",
    "]\n",
    "\n",
    "# Processar todos os datasets\n",
    "combined_results = []\n",
    "\n",
    "import os\n",
    "from all_functions import *\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "dataset = \"ANP_MONTHLY\"\n",
    "exp_name = \"SMAPE_BEST_CATEGORY_EACH_SERIE\"\n",
    "path_experiments = (\n",
    "        f\"./timeseries/mestrado/resultados/{exp_name}/\"\n",
    "    )\n",
    "path_csv = f\"{path_experiments}/{dataset}.csv\"\n",
    "os.makedirs(path_experiments, exist_ok=True)\n",
    "val_test = \"2023-11-30\"\n",
    "for i in range(0, 182):\n",
    "    best_results = {}\n",
    "    all_predictions = []\n",
    "\n",
    "    # Encontrar o melhor modelo de cada grupo\n",
    "    for group_name, models in model_groups.items():\n",
    "        val_rmse, model, preds, test = get_best_model_by_rmse(models, i, final_test,val_test, base_path)\n",
    "        best_results[group_name] = {\n",
    "            'val_rmse': val_rmse,\n",
    "            'model': model,\n",
    "            'predictions': preds\n",
    "        }\n",
    "\n",
    "        # Coletar predições válidas para combinar\n",
    "        if preds and len(preds) > 0:\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    # Combinar predições por média\n",
    "    if all_predictions:\n",
    "        # Converter para array numpy para facilitar a média\n",
    "        predictions_array = np.array(all_predictions)\n",
    "        combined_prediction = np.mean(predictions_array, axis=0).tolist()\n",
    "    else:\n",
    "        combined_prediction = []\n",
    "\n",
    "    # Encontrar o melhor entre todos os grupos\n",
    "    overall_best_group = min(best_results.items(), key=lambda x: x[1]['val_rmse'])\n",
    "\n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    print(combined_prediction)\n",
    "    # Mostrar os melhores de cada grupo\n",
    "    for group_name, result in best_results.items():\n",
    "        if result['model']:\n",
    "            print(f\"  Melhor {group_name}: {result['model']} (SMAPE: {result['val_rmse']:.4f})\")\n",
    "            \n",
    "    \n",
    "    preds_real = combined_prediction\n",
    "    \n",
    "    test = np.array(test)\n",
    "    preds_real_array = np.array(preds_real)\n",
    "    preds_real_reshaped = preds_real_array.reshape(1, -1)\n",
    "    test_reshaped = test.reshape(1, -1)\n",
    "    smape_result = calculate_smape(preds_real_reshaped, test_reshaped)\n",
    "    # print(smape_result)\n",
    "    rmse_result = calculate_rmse(preds_real_reshaped, test_reshaped)\n",
    "    msmape_result = calculate_msmape(preds_real_reshaped, test_reshaped)\n",
    "    # mase_result = calculate_mase(preds_real_reshaped, test_reshaped, training_set, seasonality)\n",
    "    mae_result = calculate_mae(preds_real_reshaped, test_reshaped)\n",
    "    mape_result = mape(test, preds_real_array)\n",
    "    pocid_result = pocid(test, preds_real_array)\n",
    "    \n",
    "    description = \"Models combined: \" + \", \".join([f\"{group}: {best_results[group]['model']}\" for group in best_results if best_results[group]['model']])\n",
    "    data_serie = {\n",
    "            \"dataset_index\": f\"{i}\",\n",
    "            \"horizon\": \"12\",\n",
    "            \"regressor\": \"BEST_CATEGORY_EACH_SERIE\",\n",
    "            \"mape\": mape_result,\n",
    "            \"pocid\": pocid_result,\n",
    "            \"smape\": smape_result,\n",
    "            \"rmse\": rmse_result,\n",
    "            \"msmape\": msmape_result,\n",
    "            \"mae\": mae_result,\n",
    "            \"test\": [test.tolist()],\n",
    "            \"predictions\": [preds_real],\n",
    "            \"start_test\": \"INICIO\",\n",
    "            \"final_test\": final_test,\n",
    "            \"description\": description,\n",
    "            # 'training_time': times[0],\n",
    "            # 'prediction_time': times[1],\n",
    "        }\n",
    "\n",
    "    if not os.path.exists(path_csv):\n",
    "            pd.DataFrame(columns=cols_serie).to_csv(path_csv, sep=\";\", index=False)\n",
    "\n",
    "    print(\"Salvando resultados...\\n\")\n",
    "    df_new = pd.DataFrame(data_serie)\n",
    "    df_new.to_csv(path_csv, sep=\";\", mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "    # print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def _ensure_array(y: Union[float, List[float]], n: int) -> np.ndarray:\n",
    "    \"\"\"Garante que y tem tamanho n; se for escalar, repete para formar array de tamanho n.\"\"\"\n",
    "    if isinstance(y, (list, tuple, np.ndarray)):\n",
    "        y_arr = np.asarray(y, dtype=float)\n",
    "        if y_arr.shape[0] != n:\n",
    "            raise ValueError(\n",
    "                f\"Lista y_val tem tamanho {y_arr.shape[0]} mas esperava {n}\"\n",
    "            )\n",
    "        return y_arr\n",
    "    else:\n",
    "        return np.full(shape=(n,), fill_value=float(y), dtype=float)\n",
    "\n",
    "\n",
    "def check_input_shapes(\n",
    "    val_preds: Dict[str, List[float]], test_preds: Dict[str, List[float]]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Verifica consistência mínima:\n",
    "    - todas as listas em val_preds têm o mesmo tamanho n_val\n",
    "    - todos os modelos em val_preds existem em test_preds (test_preds pode ter listas de tamanho diferente)\n",
    "    Retorna n_val.\n",
    "    \"\"\"\n",
    "    if not val_preds:\n",
    "        raise ValueError(\"val_preds está vazio\")\n",
    "    lengths = {k: len(v) for k, v in val_preds.items()}\n",
    "    n_vals = set(lengths.values())\n",
    "    if len(n_vals) != 1:\n",
    "        raise ValueError(f\"Tamanhos inconsistentes nas listas de validação: {lengths}\")\n",
    "    n_val = n_vals.pop()\n",
    "\n",
    "    # checar test_preds contém as chaves\n",
    "    for k in val_preds:\n",
    "        if k not in test_preds:\n",
    "            raise ValueError(\n",
    "                f\"Modelo '{k}' presente em val_preds mas ausente em test_preds\"\n",
    "            )\n",
    "        if not isinstance(test_preds[k], (list, tuple, np.ndarray)):\n",
    "            raise ValueError(f\"test_preds['{k}'] precisa ser lista/np.array\")\n",
    "    return n_val\n",
    "\n",
    "\n",
    "def shift_correction(\n",
    "    val_preds: Dict[str, List[float]],\n",
    "    y_val: List[float],\n",
    "    test_preds: Dict[str, List[float]],\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Corrige previsões adicionando um bias por modelo:\n",
    "      bias = mean(y_val) - mean(pred_val_model)\n",
    "    Aplica bias às previsões de teste e retorna dicionário de previsões corrigidas.\n",
    "    \"\"\"\n",
    "    n_val = check_input_shapes(val_preds, test_preds)\n",
    "    y_val_arr = _ensure_array(y_val, n_val)\n",
    "    y_mean = float(np.mean(y_val_arr))\n",
    "\n",
    "    corrected = {}\n",
    "    for name, preds_val in val_preds.items():\n",
    "        preds_val_arr = np.asarray(preds_val, dtype=float)\n",
    "        bias = y_mean - float(np.mean(preds_val_arr))\n",
    "        preds_test_arr = np.asarray(test_preds[name], dtype=float)\n",
    "        corrected[name] = (preds_test_arr + bias).tolist()\n",
    "\n",
    "    return corrected\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_values(list_str):\n",
    "    if isinstance(list_str, str):\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", list_str)\n",
    "        return [float(num) for num in numbers]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = {\n",
    "    \"ARIMA\": [\n",
    "        400.71678399,\n",
    "        663.87332953,\n",
    "        929.11779741,\n",
    "        928.19414546,\n",
    "        820.45108076,\n",
    "        942.73181293,\n",
    "        917.04772083,\n",
    "        777.88168678,\n",
    "        643.23803468,\n",
    "        743.95702414,\n",
    "        786.77982244,\n",
    "        730.3459295,\n",
    "    ],\n",
    "    \"FT_catboost\": [\n",
    "        400.71678399,\n",
    "        663.87332953,\n",
    "        929.11779741,\n",
    "        928.19414546,\n",
    "        820.45108076,\n",
    "        942.73181293,\n",
    "        917.04772083,\n",
    "        777.88168678,\n",
    "        643.23803468,\n",
    "        743.95702414,\n",
    "        786.77982244,\n",
    "        730.3459295,\n",
    "    ],\n",
    "    \"ONLY_DWT_rf\": [\n",
    "        355.62580122,\n",
    "        450.92858957,\n",
    "        417.68229707,\n",
    "        1332.2369491,\n",
    "        529.15589532,\n",
    "        921.31116252,\n",
    "        974.07278312,\n",
    "        809.75277656,\n",
    "        747.63653457,\n",
    "        549.20459147,\n",
    "        1694.72720717,\n",
    "        1188.96124668,\n",
    "    ],\n",
    "    \"FT_svr\": [\n",
    "        480.87089365,\n",
    "        520.23308461,\n",
    "        632.27912708,\n",
    "        657.58230609,\n",
    "        471.22153901,\n",
    "        522.99862274,\n",
    "        496.83943868,\n",
    "        477.60744965,\n",
    "        398.98449469,\n",
    "        337.1191924,\n",
    "        405.94991964,\n",
    "        352.95463704,\n",
    "    ],\n",
    "    \"NaiveSeasonal\": [\n",
    "        163.35,\n",
    "        124.32,\n",
    "        1767.006,\n",
    "        1003.093,\n",
    "        76.45,\n",
    "        2023.998,\n",
    "        2251.496,\n",
    "        1055.33,\n",
    "        245.87,\n",
    "        162.73,\n",
    "        670.983,\n",
    "        -1.13686838e-13,\n",
    "    ],\n",
    "}\n",
    "y_val = [\n",
    "    47.51,\n",
    "    0.0,\n",
    "    62.15,\n",
    "    4214.105,\n",
    "    92.68,\n",
    "    1030.353,\n",
    "    2585.757,\n",
    "    1267.991,\n",
    "    0.0,\n",
    "    2517.924,\n",
    "    855.162,\n",
    "    250.91,\n",
    "]\n",
    "\n",
    "test_preds = {\n",
    "    \"ARIMA\": [\n",
    "        626.54445435,\n",
    "        867.25753704,\n",
    "        1140.44178255,\n",
    "        1218.40255972,\n",
    "        922.16557401,\n",
    "        1025.36568259,\n",
    "        1277.64676113,\n",
    "        1043.62573386,\n",
    "        871.6622963,\n",
    "        977.16178989,\n",
    "        938.74572533,\n",
    "        920.28328892,\n",
    "    ],\n",
    "    \"FT_catboost\": [\n",
    "        287.72484798,\n",
    "        -296.80893321,\n",
    "        259.47301569,\n",
    "        -221.27725565,\n",
    "        255.15551412,\n",
    "        254.38388207,\n",
    "        203.31202369,\n",
    "        793.22870178,\n",
    "        1008.24206288,\n",
    "        1084.23653574,\n",
    "        554.45271123,\n",
    "        564.5496957,\n",
    "    ],\n",
    "    \"ONLY_DWT_rf\": [\n",
    "        1167.69032389,\n",
    "        718.71243323,\n",
    "        352.10670718,\n",
    "        844.43235289,\n",
    "        2291.78410799,\n",
    "        889.53650482,\n",
    "        993.63343095,\n",
    "        1258.16041135,\n",
    "        595.12303392,\n",
    "        1212.79863338,\n",
    "        1000.14051588,\n",
    "        2113.93667613,\n",
    "    ],\n",
    "    \"FT_svr\": [\n",
    "        -387.22267965,\n",
    "        -1602.8661116,\n",
    "        -1434.76910943,\n",
    "        -1765.4953112,\n",
    "        -561.58789508,\n",
    "        353.94495089,\n",
    "        816.7784578,\n",
    "        971.05251116,\n",
    "        649.41751243,\n",
    "        1055.17371582,\n",
    "        282.25494992,\n",
    "        149.97195324,\n",
    "    ],\n",
    "    \"NaiveSeasonal\": [\n",
    "        47.51,\n",
    "        -2.27373675e-13,\n",
    "        62.15,\n",
    "        4214.105,\n",
    "        92.68,\n",
    "        1030.353,\n",
    "        2585.757,\n",
    "        1267.991,\n",
    "        -2.27373675e-13,\n",
    "        2517.924,\n",
    "        855.162,\n",
    "        250.91,\n",
    "    ],\n",
    "}\n",
    "y_test = [2101.917,\n",
    " 4194.392,\n",
    " 4443.881,\n",
    " 3153.49,\n",
    " 1398.42,\n",
    " 680.292,\n",
    " -3.036,\n",
    " 2000.117,\n",
    " 2160.356,\n",
    " 1995.957,\n",
    " 4128.637,\n",
    " 4555.324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.51127291866553"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "new_preds = linear_correction(val_preds=val_preds, y_val=y_val, test_preds=test_preds)\n",
    "\n",
    "preds_list = [preds for preds in new_preds.values()]\n",
    "combined = np.mean(preds_list, axis=0)\n",
    "mape(y_test, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[673.091,\n",
       " 645.903,\n",
       " 2587.536,\n",
       " 1970.165,\n",
       " 3338.866,\n",
       " 642.657,\n",
       " 328.47,\n",
       " 290.66,\n",
       " 729.046,\n",
       " 654.321,\n",
       " 1048.09,\n",
       " 719.027]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "index = 12\n",
    "df_arima = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/NaiveSeasonal/normal/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_arima = df_arima[df_arima[\"final_test\"] == \"2020-11-30\"]\n",
    "df_arima = df_arima[df_arima[\"dataset_index\"] == index]\n",
    "extract_values(df_arima['test'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[626.54445435,\n",
       " 867.25753704,\n",
       " 1140.44178255,\n",
       " 1218.40255972,\n",
       " 922.16557401,\n",
       " 1025.36568259,\n",
       " 1277.64676113,\n",
       " 1043.62573386,\n",
       " 871.6622963,\n",
       " 977.16178989,\n",
       " 938.74572533,\n",
       " 920.28328892]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index = 12\n",
    "df_arima = pd.read_csv(\n",
    "    \"./timeseries/mestrado/resultados/ARIMA/normal/ANP_MONTHLY.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "# pegar cada valor das colunas mape, pocid, smape, rmse, msmape, mae e gerar uma media de todos\n",
    "df_arima = df_arima[df_arima[\"final_test\"] == \"2024-11-30\"]\n",
    "df_arima = df_arima[df_arima[\"dataset_index\"] == index]\n",
    "extract_values(df_arima[\"predictions\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
