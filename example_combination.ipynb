{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/lucas/lib/python3.11/site-packages/rapids_dask_dependency/dask_loader.py:36: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 11.0.0. Please consider upgrading.\n",
      "  mod = importlib.import_module(spec.name)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U aeon\n",
    "#!pip install aeon[all_extras]\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from aeon.visualisation import plot_series\n",
    "from all_functions import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from aeon.transformations.detrend import STLTransformer\n",
    "import ast\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import os\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "def convert_to_list(series_str):\n",
    "    return eval(series_str)\n",
    "\n",
    "def get_preds_hybrid(path, test_date, start_index):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    results = {}\n",
    "    filtered_df = df[df['test_range'] == test_date]\n",
    "    columns_p1_to_p12 = filtered_df.loc[:, 'P1':'P12']\n",
    "    values_list = columns_p1_to_p12.values.flatten().tolist()     \n",
    "    results = pd.Series(values_list, index=start_index)\n",
    "    return results\n",
    "\n",
    "def generate_index(start_date, end_date):\n",
    "    end_date_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    start_date_dt = pd.to_datetime(start_date)\n",
    "    \n",
    "    index = pd.period_range(start=start_date_dt, end=end_date_dt, freq='M')\n",
    "\n",
    "    return index\n",
    "\n",
    "def get_each_point_error(path, val_date):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    \n",
    "    df_filtered = df[df['test_range'] <= val_date].copy()\n",
    "\n",
    "    df_filtered = df_filtered.iloc[::-1]\n",
    "    \n",
    "    error_series_concatenated = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        error_series_str = row['error_series']\n",
    "        error_list = ast.literal_eval(error_series_str)\n",
    "        error_series_concatenated.append(error_list)\n",
    "    return error_series_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95083.29635088 95975.46774454 94948.44731053 94904.82073359]\n",
      "[95630.60624758 95510.20676352 95879.93712835 95798.8512155\n",
      " 95896.2895904  95905.00814599]\n",
      "[95107.51605042 95905.00814599]\n",
      "[92719.76690981 95256.47512741]\n",
      "[94380.82249293 94903.30097995 94764.1018085  95090.09232506]\n",
      "[91973.9639792  92396.92527536 91848.91721162 92039.51011661\n",
      " 91728.30037531 92068.30326817]\n",
      "[94118.2421303  94111.40845078]\n",
      "[91962.26220054 91453.24959009]\n",
      "[95739.29218841 96007.9846254  94302.06680264 96423.28487189]\n",
      "[93699.80083637 93141.03720674 93566.08043297 93432.24709444\n",
      " 93359.52131989 93625.51643664]\n",
      "[94606.09345668 96018.57971621]\n",
      "[92225.13507427 94483.48093897]\n",
      "[95464.64462365 96045.33189272 95316.14478465 94709.85188788]\n",
      "[92705.13892762 92669.25710575 93034.75324912 93100.98330763\n",
      " 92867.78595674 93214.75999989]\n",
      "[94570.48838742 95265.74137258]\n",
      "[93358.30363319 92114.16201392]\n",
      "[ 99042.51417315  99826.4628604   99099.34142092 100991.24494543]\n",
      "[97750.63372578 97923.16729785 97777.72897954 97823.66214186\n",
      " 97713.89011917 97898.41560204]\n",
      "[98400.28297471 99428.51986472]\n",
      "[98034.56419037 97444.785589  ]\n",
      "[98557.76045552 99385.59365706 98362.27862259 99267.53185588]\n",
      "[98486.44158243 97946.57313961 98689.49318711 98557.18983535\n",
      " 98639.70302358 98614.31436684]\n",
      "[97841.11197908 99457.17042747]\n",
      "[96590.77962802 98562.95624119]\n",
      "[ 98662.98016749  99114.35996148  99414.97172446 100147.73349891]\n",
      "[97443.56453931 97494.5175013  97813.81364051 97895.99281844\n",
      " 97761.6960785  97908.3100792 ]\n",
      "[97748.80804285 97138.34407994]\n",
      "[96669.70819177 91689.63245178]\n",
      "[100386.3261623  100547.16645465 100873.24124461 100168.64247868]\n",
      "[100748.21075086 100711.45017601 100676.43276769 100584.41550095\n",
      " 100685.64870129 100654.35264837]\n",
      "[99726.80869083 99723.30283722]\n",
      "[98705.91253636 97623.09343511]\n",
      "[97056.53684718 97166.79494015 96873.84281914 96401.01179025]\n",
      "[96326.98086651 95738.02759828 96823.48671523 96858.10620465\n",
      " 96830.7725069  96820.71170146]\n",
      "[96138.22885057 95671.18704304]\n",
      "[94548.42877781 94622.24050659]\n",
      "[101741.27193071 102113.09504176 102357.8564735  101561.3450328 ]\n",
      "[102539.64761354 101971.12640098 102841.30470744 102727.87295328\n",
      " 102941.94512852 102725.98424886]\n",
      "[100764.64934896 101225.83038613]\n",
      "[ 99983.61262425 100132.24876579]\n",
      "[95971.54105109 96186.3820786  96562.51774963 94890.53957071]\n",
      "[95840.04399626 95136.81996889 96344.27237473 96257.77276143\n",
      " 96368.57643939 96254.41490141]\n",
      "[95215.06388351 95019.4450702 ]\n",
      "[93045.22592263 93617.20135137]\n",
      "[92654.77249454 92714.28005172 93081.89295294 91820.38816626]\n",
      "[91354.30499099 90505.79968078 91676.64811298 91698.46647093\n",
      " 91645.54424027 91720.90583859]\n",
      "[91801.59755144 90673.73233107]\n",
      "[89822.40700298 87607.967401  ]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_preds_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 217\u001b[0m\n\u001b[1;32m    215\u001b[0m             final_prediction\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(cluster_means)) \n\u001b[1;32m    216\u001b[0m         \u001b[38;5;66;03m# mean_preds_year = final_prediction\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     all_preds_dates\u001b[38;5;241m.\u001b[39mextend(mean_preds_year)\n\u001b[1;32m    219\u001b[0m rmse_result \u001b[38;5;241m=\u001b[39m rmse(all_test, all_preds_dates)\n\u001b[1;32m    220\u001b[0m mape_result \u001b[38;5;241m=\u001b[39m mape(all_test, all_preds_dates)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_preds_year' is not defined"
     ]
    }
   ],
   "source": [
    "# models = [\"ETS\",\"catboost\", \"arima\", \"rf\"]\n",
    "# transformations = [\"normal\", \"deseasonal\"]\n",
    "from dtaidistance import dtw\n",
    "from scipy.stats import norm\n",
    "from tslearn.barycenters import \\\n",
    "    euclidean_barycenter, \\\n",
    "    dtw_barycenter_averaging, \\\n",
    "    dtw_barycenter_averaging_subgradient, \\\n",
    "    softdtw_barycenter\n",
    "\n",
    "from aeon.clustering.averaging import elastic_barycenter_average\n",
    "from minisom import MiniSom\n",
    "dirs = [\n",
    "    '../datasets/venda/mensal/uf/gasolinac/',\n",
    "    '../datasets/venda/mensal/uf/etanolhidratado/',\n",
    "    '../datasets/venda/mensal/uf/glp/',\n",
    "    '../datasets/venda/mensal/uf/oleodiesel/',\n",
    "    '../datasets/venda/mensal/uf/querosenedeaviacao/',\n",
    "]\n",
    "horizon = 12\n",
    "window = 12\n",
    "model_transformation = [\n",
    "                        \"GASF_ridge_deseasonal\",\n",
    "                        \"GADF_ridge_deseasonal\",\n",
    "                        \"STFT_ridge_deseasonal\",\n",
    "                        \"MTF_ridge_deseasonal\",\n",
    "                        \"RP_ridge_deseasonal\",\n",
    "                        \"FIRTS_ridge_deseasonal\",\n",
    "                        \"DWT_ridge_deseasonal\",\n",
    "                        \"SWT_ridge_deseasonal\",\n",
    "                        \"CWT_ridge_deseasonal\",\n",
    "                        \"ETS_normal\",\n",
    "                        \"WPT_ridge_deseasonal\",\n",
    "                        \"catboost_deseasonal\",\n",
    "                        \"ridge_deseasonal\",\n",
    "                        \"arima_deseasonal\"\n",
    "                        ]\n",
    "dates = [\n",
    "        # '1993-03_1994-02',\n",
    "        # '1994-03_1995-02', '1995-03_1996-02', '1996-03_1997-02', '1997-03_1998-02', '1998-03_1999-02',\n",
    "        # '1999-03_2000-02', '2000-03_2001-02', '2001-03_2002-02', '2002-03_2003-02', '2003-03_2004-02',\n",
    "        # '2004-03_2005-02', '2005-03_2006-02', '2006-03_2007-02', '2007-03_2008-02', '2008-03_2009-02',\n",
    "        # '2009-03_2010-02', '2010-03_2011-02', '2011-03_2012-02', '2012-03_2013-02', '2013-03_2014-02',\n",
    "        # '2014-03_2015-02', '2015-03_2016-02', '2016-03_2017-02', '2017-03_2018-02', '2018-03_2019-02',\n",
    "        '2019-03_2020-02', '2020-03_2021-02', '2021-03_2022-02', '2022-03_2023-02', '2023-03_2024-02',\n",
    "         ]\n",
    "\n",
    "\n",
    "date_past = {\n",
    "            '2019-03_2020-02': '2018-03_2019-02',\n",
    "            '2020-03_2021-02': '2019-03_2020-02',\n",
    "            '2021-03_2022-02': '2020-03_2021-02',\n",
    "            '2022-03_2023-02': '2021-03_2022-02',\n",
    "            '2023-03_2024-02': '2022-03_2023-02',\n",
    "             }\n",
    "\n",
    "produto = \"gasolinac\"\n",
    "estado = \"SP\"\n",
    "\n",
    "combination_format = \"som\"\n",
    "for directory in dirs:\n",
    "    all_rmse = []\n",
    "    all_pocid = []\n",
    "    all_mape = []\n",
    "    all_pbe = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            uf = file.split(\"_\")[1].upper()\n",
    "            derivado = file.split(\"_\")[2].split(\".\")[0]\n",
    "            \n",
    "            # if uf == estado and produto == derivado:\n",
    "            full_path = os.path.join(directory, file)\n",
    "            series = read_series(full_path)\n",
    "            \n",
    "            all_preds_dates = []\n",
    "            all_test = []\n",
    "\n",
    "\n",
    "            for date in dates:\n",
    "                start_date, end_date = date.split('_')\n",
    "                test_index = generate_index(start_date, end_date)\n",
    "                test_real = get_test_real(series, start_date, end_date)\n",
    "                row_data = []\n",
    "                all_preds_year = []\n",
    "\n",
    "                preds_barycenter = []\n",
    "                preds_elastic = []\n",
    "                \n",
    "                for mt in model_transformation:\n",
    "                    model, transform = mt.rsplit(\"_\", 1)\n",
    "\n",
    "                    path_result = f'./paper_roma/{model}/{derivado}/{transform}/transform_{uf}.csv'\n",
    "\n",
    "                    serie_pred = get_preds_hybrid(path_result, date, test_index)\n",
    "\n",
    "                    #com base no erro\n",
    "                    # error_past = get_each_point_error(path_result, date_past[date])[0]\n",
    "                    # mean_error = abs(sum(error_past) / len(error_past))\n",
    "                    # mean_error = abs(np.mean(error_past))\n",
    "                    # std_error = np.std(error_past)\n",
    "\n",
    "                    # lower = [pred - mean_error for pred in serie_pred.tolist()]\n",
    "                    # upper = [pred + mean_error for pred in serie_pred.tolist()]\n",
    "                    # lower_mean = [pred - mean_error for pred in serie_pred]\n",
    "                    # upper_mean = [pred + mean_error for pred in serie_pred]\n",
    "\n",
    "                    # z = norm.ppf(0.95)\n",
    "                    # lower = [pred - z * std_error for pred in serie_pred]\n",
    "                    # upper = [pred + z * std_error for pred in serie_pred]\n",
    "\n",
    "\n",
    "                    # lower_upper = [lower, upper]\n",
    "                    # result_lower_upper = dtw_barycenter_averaging_subgradient(lower_upper, max_iter=35)\n",
    "                    # result_lower_upper = result_lower_upper.flatten().tolist()\n",
    "                    # preds_barycenter.append(result_lower_upper)\n",
    "                    # all_preds_year.append(result_lower_upper)\n",
    "\n",
    "                    # preds_barycenter.append(upper)\n",
    "                    # preds_barycenter.append(lower)\n",
    "\n",
    "                    # all_preds_year.append(upper)\n",
    "                    # all_preds_year.append(lower)\n",
    "\n",
    "                    # adiciona 10% para intervalo de confianca\n",
    "                    # alpha = 0.1\n",
    "                    # lower = [p - alpha * p for p in serie_pred.tolist()]\n",
    "                    # upper = [p + alpha * p for p in serie_pred.tolist()]\n",
    "\n",
    "                    # print(mt)\n",
    "                    # print(upper)\n",
    "                    # print(serie_pred.tolist())\n",
    "                    # print(lower)\n",
    "                    # print(\"--------\")\n",
    "\n",
    "\n",
    "                    \n",
    "                    # print(serie_pred)\n",
    "                    all_preds_year.append(serie_pred.tolist())\n",
    "                    \n",
    "\n",
    "                    preds_barycenter.append(serie_pred.tolist())\n",
    "                    \n",
    "                    preds_elastic.append(np.array([[serie_pred.tolist()]]))\n",
    "                    \n",
    "                    # preds_elastic.append(np.array([[lower]]))\n",
    "                    # preds_elastic.append(np.array([[upper]]))\n",
    "                    # preds_elastic.append(np.array([[result_lower_upper]]))\n",
    "                    # preds_elastic.append(np.array([[serie_pred.values]]))\n",
    "                \n",
    "                all_test.extend(test_real)\n",
    "                \n",
    "                if combination_format == \"mean\":\n",
    "                    mean_preds_year = np.mean(np.array(all_preds_year), axis=0)\n",
    "                elif combination_format == \"median\":\n",
    "                    mean_preds_year = np.median(np.array(all_preds_year), axis=0)\n",
    "                elif combination_format == \"softdtw\":\n",
    "                    #softdtw barycenter\n",
    "                    softdtw_preds = softdtw_barycenter(preds_barycenter, max_iter=35, gamma=0.01)\n",
    "                    mean_preds_year = softdtw_preds.flatten().tolist()\n",
    "                elif combination_format == \"dtw_subgradient\":\n",
    "                    #dtw avg barycenter subradient\n",
    "                    preds_dtw_subgradient = dtw_barycenter_averaging_subgradient(preds_barycenter, max_iter=35, random_state=0)\n",
    "                    mean_preds_year = preds_dtw_subgradient.flatten().tolist()\n",
    "\n",
    "                elif combination_format == \"dtw_subgradient2\":\n",
    "                    #dtw avg barycenter subradient\n",
    "                    metric_params = {\n",
    "                                        \"global_constraint\": \"itakura\",\n",
    "                                        \"sakoe_chiba_radius\": 6, \n",
    "                                        \"itakura_max_slope\": 1.5\n",
    "                                    }\n",
    "                    \n",
    "                    # init = np.median(np.array(all_preds_year), axis=0)\n",
    "                    # init = init.reshape(1, -1).T\n",
    "                    shape = 'normal'\n",
    "                    init = None\n",
    "                    X = np.array(preds_barycenter).reshape((len(preds_barycenter)), len(preds_barycenter[0]), 1)\n",
    "                    \n",
    "                    preds_dtw_subgradient = dtw_barycenter_averaging_subgradient(X, init_barycenter=init, max_iter=35, random_state=0, metric_params=metric_params)\n",
    "                    mean_preds_year = preds_dtw_subgradient.flatten().tolist()\n",
    "\n",
    "\n",
    "                elif combination_format == \"dtw\":\n",
    "                    #dtw barycenter averaging\n",
    "                    preds_dtw_avg = dtw_barycenter_averaging(preds_barycenter, max_iter=35)\n",
    "                    mean_preds_year = preds_dtw_avg.flatten().tolist()\n",
    "                \n",
    "                elif combination_format == \"eba\":\n",
    "                    #elastic barycenter average\n",
    "                    capt = np.vstack(preds_elastic)\n",
    "                    shape = \"twe\"\n",
    "                    # X = np.array(preds_elastic).reshape(len(preds_elastic), 1, 12)\n",
    "                    # init = np.median(np.array(all_preds_year), axis=0)\n",
    "                    # init = init.reshape(1, -1)\n",
    "                    init = 'random'\n",
    "                    elastic = elastic_barycenter_average(capt, distance=shape, init_barycenter=init, random_state=0),\n",
    "                    mean_preds_year = elastic[0].tolist()[0]\n",
    "                elif combination_format == \"som\":\n",
    "                    all_preds_year = np.array(all_preds_year)\n",
    "                    shape = 'mean'\n",
    "                    som_width, som_height = 2, 2\n",
    "                    som = MiniSom(som_width, som_height, input_len=all_preds_year.shape[1], sigma=0.8, learning_rate=0.3, random_seed=0)\n",
    "                    som.random_weights_init(all_preds_year)\n",
    "                    som.train_random(all_preds_year, 200)\n",
    "                    bmu_indices = np.array([som.winner(x) for x in all_preds_year])\n",
    "\n",
    "                    final_prediction = []\n",
    "                    for i in range(all_preds_year.shape[1]):\n",
    "                        cluster_means = []\n",
    "                        for x, y in np.unique(bmu_indices, axis=0):\n",
    "                            cluster_data = all_preds_year[(bmu_indices[:, 0] == x) & (bmu_indices[:, 1] == y), i]\n",
    "                            if len(cluster_data) > 0:\n",
    "                                cluster_means.append(cluster_data.mean())\n",
    "                                print(cluster_data)\n",
    "                        final_prediction.append(np.mean(cluster_means)) \n",
    "                    # mean_preds_year = final_prediction\n",
    "                all_preds_dates.extend(mean_preds_year)\n",
    "\n",
    "            rmse_result = rmse(all_test, all_preds_dates)\n",
    "            mape_result = mape(all_test, all_preds_dates)\n",
    "            pocid_result = pocid(all_test, all_preds_dates)\n",
    "            pbe_result = pbe(pd.Series(all_test), pd.Series(all_preds_dates))\n",
    "            mcpm_result = mcpm(rmse_result, mape_result, pocid_result)\n",
    "\n",
    "            all_pocid.append(pocid_result)\n",
    "            all_pbe.append(pbe_result)\n",
    "            all_rmse.append(rmse_result)\n",
    "            all_mape.append(mape_result)\n",
    "        \n",
    "    \n",
    "\n",
    "    if len(all_rmse) > 0:\n",
    "        media_rmse = sum(all_rmse) / len(all_rmse)\n",
    "        media_mape = sum(all_mape) / len(all_mape)\n",
    "        media_pocid = sum(all_pocid) / len(all_pocid)\n",
    "        media_pbe = sum(all_pbe) / len(all_pbe)\n",
    "\n",
    "        representacao = f'{combination_format}_{shape}'\n",
    "\n",
    "        data = {\n",
    "            \"representacao\": [representacao],\n",
    "            \"RMSE\": [media_rmse],\n",
    "            \"MAPE\": [media_mape],\n",
    "            \"POCID\": [media_pocid],\n",
    "            \"PBE\": [media_pbe],\n",
    "            \"derivado\": [derivado]\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        csv_file = f'./metrics2/{derivado}.csv'\n",
    "        os.makedirs('./metrics2/', exist_ok=True)\n",
    "\n",
    "        df.to_csv(csv_file, mode='a', header=not pd.io.common.file_exists(csv_file), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lucas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
